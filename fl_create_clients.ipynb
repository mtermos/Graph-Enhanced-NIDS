{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84cb9753-9962-4cfa-b78f-ed13216e3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# datasets is a list of available datasets descriptions containing: path, key columns names, and suitable complex network features\n",
    "from src.dataset.dataset_info import datasets, cn_measures_type_1, cn_measures_type_2, cn_measures_type_3, cn_measures_type_4, network_features_type_1,network_features_type_2,network_features_type_3,network_features_type_4\n",
    "from src.graph.graph_measures import calculate_graph_measures\n",
    "from src.graph.centralities.add_centralities import add_centralities\n",
    "from src.dataset.add_pca_columns import process_clients_with_grouped_pca, evaluate_pca_results, process_clients_with_pca, process_clients_with_grouped_pca_rmse\n",
    "\n",
    "\n",
    "with_sort_timestamp = True\n",
    "undersample_classes = True\n",
    "folder_path = \"temp/\"\n",
    "folder_path_prep = \"temp/preprocessed/\"\n",
    "output_folder = 'datasets/gdlc'\n",
    "# output_folder = 'datasets/dbp'\n",
    "\n",
    "\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "    os.mkdir(folder_path_prep)\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ca3130",
   "metadata": {},
   "source": [
    "# Preparing Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43cecea5",
   "metadata": {},
   "source": [
    "### Reading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8e652c-7d74-47a3-9faa-7da0f290d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> dataset1.name: cic_ton_iot_5_percent\n"
     ]
    }
   ],
   "source": [
    "dataset1 = datasets[\"cic_ton_iot_5_percent\"]\n",
    "df1 = pd.read_parquet(dataset1.path)\n",
    "\n",
    "dataset2 = datasets[\"cic_ids_2017_5_percent\"]\n",
    "df2 = pd.read_parquet(dataset2.path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b147094b",
   "metadata": {},
   "source": [
    "### Attacks Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98616b2-7166-4702-a1ae-c00ab83f4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benign' 'xss' 'password' 'scanning' 'injection' 'ransomware' 'mitm'\n",
      " 'backdoor' 'ddos' 'dos']\n"
     ]
    }
   ],
   "source": [
    "classes1 = df1[dataset1.class_col].unique()\n",
    "print(classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd28659f-d720-4b8b-ac2f-aa4eb5dd279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes2: ['BENIGN' 'PortScan' 'DoS Hulk' 'DoS slowloris' 'DDoS' 'DoS Slowhttptest'\n",
      " 'FTP-Patator' 'SSH-Patator' 'DoS GoldenEye' 'Web Attack � Brute Force'\n",
      " 'Infiltration' 'Bot' 'Web Attack � XSS' 'Web Attack � Sql Injection']\n"
     ]
    }
   ],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adc262cb",
   "metadata": {},
   "source": [
    "renaming some attacks to fit the naming in df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c52211-e107-49d3-a5a3-a2d942abad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[dataset2.class_col] = df2[dataset2.class_col].replace({\"BENIGN\": \"Benign\",\n",
    "                                                            \"DDoS\": \"ddos\",\n",
    "                                                            \"Web Attack � Brute Force\": \"bruteforce\",\n",
    "                                                            \"Web Attack � XSS\": \"xss\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93132189-7c75-48e0-a13a-d2a75f463ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes2: ['Benign' 'PortScan' 'DoS Hulk' 'DoS slowloris' 'ddos' 'DoS Slowhttptest'\n",
      " 'FTP-Patator' 'SSH-Patator' 'DoS GoldenEye' 'bruteforce' 'Infiltration'\n",
      " 'Bot' 'xss' 'Web Attack � Sql Injection']\n"
     ]
    }
   ],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9bd293c-061e-4e4d-968f-28d459bea765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes: {'scanning', 'xss', 'Infiltration', 'Benign', 'PortScan', 'DoS Hulk', 'Bot', 'dos', 'password', 'DoS slowloris', 'bruteforce', 'FTP-Patator', 'backdoor', 'DoS Slowhttptest', 'mitm', 'DoS GoldenEye', 'injection', 'ransomware', 'SSH-Patator', 'ddos', 'Web Attack � Sql Injection'}\n"
     ]
    }
   ],
   "source": [
    "classes = set(np.concatenate([classes2,classes1]))\n",
    "print(f\"==>> classes: {classes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dfea377",
   "metadata": {},
   "source": [
    "### Sorting (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f48d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_sort_timestamp:\n",
    "    df1[dataset1.timestamp_col] = pd.to_datetime(df1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    df1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    df2[dataset2.timestamp_col] = pd.to_datetime(df2[dataset2.timestamp_col].str.strip(), format=dataset2.timestamp_format)\n",
    "    df2.sort_values(dataset2.timestamp_col, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1215ca63",
   "metadata": {},
   "source": [
    "### Encoding Attacks into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b820edb5-7f77-4a02-8305-9491e3c8a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> labels_names: {0: 'Benign', 1: 'Bot', 2: 'DoS GoldenEye', 3: 'DoS Hulk', 4: 'DoS Slowhttptest', 5: 'DoS slowloris', 6: 'FTP-Patator', 7: 'Infiltration', 8: 'PortScan', 9: 'SSH-Patator', 10: 'Web Attack � Sql Injection', 11: 'backdoor', 12: 'bruteforce', 13: 'ddos', 14: 'dos', 15: 'injection', 16: 'mitm', 17: 'password', 18: 'ransomware', 19: 'scanning', 20: 'xss'}\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(classes))\n",
    "\n",
    "df1[dataset1.class_num_col] = label_encoder.transform(df1[dataset1.class_col])\n",
    "df2[dataset2.class_num_col] = label_encoder.transform(df2[dataset2.class_col])\n",
    "labels_names = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
    "\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd1c995b",
   "metadata": {},
   "source": [
    "### Undersampling classes (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5cbab61-3bd1-43c1-892c-4db0c150f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign        125686\n",
      "xss           107651\n",
      "password       16850\n",
      "injection      13876\n",
      "scanning        1768\n",
      "backdoor        1380\n",
      "ransomware       260\n",
      "mitm              30\n",
      "dos                6\n",
      "ddos               5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71fd9f7c-aa56-477a-9f24-4b9c4bf63826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_label: Benign\n",
      "==>> class_label: xss\n",
      "==>> class_label: password\n",
      "==>> class_label: injection\n",
      "==>> class_label: scanning\n",
      "==>> class_label: backdoor\n",
      "==>> class_label: ransomware\n",
      "==>> class_label: mitm\n",
      "==>> class_label: dos\n",
      "==>> class_label: ddos\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:2]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        print(f\"==>> class_label: {class_label}\")\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df1[df1[dataset1.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df1[df1[dataset1.class_col] == class_label])\n",
    "\n",
    "    df1 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df1 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eceb2754-3b56-467b-8470-9ab1c832f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign        62843\n",
      "xss           53826\n",
      "password      16850\n",
      "injection     13876\n",
      "scanning       1768\n",
      "backdoor       1380\n",
      "ransomware      260\n",
      "mitm             30\n",
      "dos               6\n",
      "ddos              5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "322651d3-96ef-4ba0-aa6a-dfadd19030d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign                        113501\n",
      "DoS Hulk                       11512\n",
      "PortScan                        7916\n",
      "ddos                            6391\n",
      "DoS GoldenEye                    519\n",
      "FTP-Patator                      377\n",
      "SSH-Patator                      289\n",
      "DoS slowloris                    283\n",
      "DoS Slowhttptest                 259\n",
      "Bot                               87\n",
      "bruteforce                        76\n",
      "xss                               36\n",
      "Infiltration                       6\n",
      "Web Attack � Sql Injection         3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0459c6e2-6a02-40f9-a26c-273311f10e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:1]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df2[df2[dataset2.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df2[df2[dataset2.class_col] == class_label])\n",
    "\n",
    "    df2 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df2 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "210b63b7-cc3d-4b7b-921e-62ce34426204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign                        56750\n",
      "DoS Hulk                      11512\n",
      "PortScan                       7916\n",
      "ddos                           6391\n",
      "DoS GoldenEye                   519\n",
      "FTP-Patator                     377\n",
      "SSH-Patator                     289\n",
      "DoS slowloris                   283\n",
      "DoS Slowhttptest                259\n",
      "Bot                              87\n",
      "bruteforce                       76\n",
      "xss                              36\n",
      "Infiltration                      6\n",
      "Web Attack � Sql Injection        3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90723f84",
   "metadata": {},
   "source": [
    "### saving labels encodings and datasets properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0e2c1f3-facf-492a-a6b4-6ef70cb008eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + '/labels_names.pkl', 'wb') as f:\n",
    "    pickle.dump([labels_names, classes], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30b4c519-7e2a-4441-8101-73615e352968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'cic_ton_iot_5_percent',\n",
       " 'length': 150844,\n",
       " 'num_benign': 62843,\n",
       " 'percentage_of_benign_records': 41.66092121662115,\n",
       " 'num_attack': 88001,\n",
       " 'percentage_of_attack_records': 58.33907878337885,\n",
       " 'attacks': ['xss',\n",
       "  'Benign',\n",
       "  'injection',\n",
       "  'ransomware',\n",
       "  'password',\n",
       "  'backdoor',\n",
       "  'scanning',\n",
       "  'mitm',\n",
       "  'ddos',\n",
       "  'dos'],\n",
       " 'number_of_nodes': 12283,\n",
       " 'number_of_edges': 12895}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = len(df1)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df1[df1['Label'] == 0])\n",
    "num_attack = len(df1[df1['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df1[\"Attack\"].unique()) \n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df1,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "properties[\"number_of_nodes\"] = G.number_of_nodes() \n",
    "properties[\"number_of_edges\"] = G.number_of_edges()\n",
    "\n",
    "with open(folder_path + '/df1_properties.txt', 'w') as f:\n",
    "    json.dump(properties, f)\n",
    "    \n",
    "properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b18d60a-c1bf-47af-8b06-72c19a2d63d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'cic_ton_iot_5_percent',\n",
       " 'length': 84504,\n",
       " 'num_benign': 56750,\n",
       " 'percentage_of_benign_records': 67.1565843037016,\n",
       " 'num_attack': 27754,\n",
       " 'percentage_of_attack_records': 32.8434156962984,\n",
       " 'attacks': ['Benign',\n",
       "  'ddos',\n",
       "  'DoS GoldenEye',\n",
       "  'PortScan',\n",
       "  'DoS Hulk',\n",
       "  'DoS slowloris',\n",
       "  'FTP-Patator',\n",
       "  'Bot',\n",
       "  'DoS Slowhttptest',\n",
       "  'SSH-Patator',\n",
       "  'xss',\n",
       "  'bruteforce',\n",
       "  'Infiltration',\n",
       "  'Web Attack � Sql Injection'],\n",
       " 'number_of_nodes': 7520,\n",
       " 'number_of_edges': 16778}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = len(df2)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df2[df2['Label'] == 0])\n",
    "num_attack = len(df2[df2['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df2[\"Attack\"].unique())  # .to_list()\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df2,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "properties[\"number_of_nodes\"] = G.number_of_nodes() \n",
    "properties[\"number_of_edges\"] = G.number_of_edges()\n",
    "\n",
    "with open(folder_path + '/df2_properties.txt', 'w') as f:\n",
    "    json.dump(properties, f)\n",
    "\n",
    "properties\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df8ce8b0",
   "metadata": {},
   "source": [
    "# Splitting into Clients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e10cf8bc",
   "metadata": {},
   "source": [
    "### creating main training and testing splits\n",
    "\n",
    "test parts will be concatenated to create the main testing df\n",
    "\n",
    "train parts will be further splitted into clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ff965d8-b49c-4bee-80a4-fc522d9d7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(df1, test_size=0.1, shuffle= True, random_state=1, stratify=df1[dataset1.class_col])\n",
    "train2, test2 = train_test_split(df2, test_size=0.1, shuffle= True, random_state=1, stratify=df2[dataset2.class_col])\n",
    "\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    # train1[dataset1.timestamp_col] = pd.to_datetime(train1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    train1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "    train2.sort_values(dataset2.timestamp_col, inplace= True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ab56873",
   "metadata": {},
   "source": [
    "### Computing graph-level measures (to apply GDLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8c21e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtermos\\Desktop\\Graph-Enhanced-NIDS\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# split dfs into clients\n",
    "client_data = np.array_split(train1, 5) + np.array_split(train2, 3)\n",
    "\n",
    "graphs_properties_path = os.path.join(output_folder, 'graphs_properties')\n",
    "\n",
    "for cid, data_partition in enumerate(client_data):\n",
    "    data_partition.to_parquet(\n",
    "        folder_path + \"client_{}.parquet\".format(cid))\n",
    "\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        data_partition, source=dataset1.src_ip_col, target=dataset1.dst_ip_col, create_using=nx.Graph())\n",
    "    properties = calculate_graph_measures(G, f\"client_{cid}\", graphs_properties_path)\n",
    "    print(f\"Computed properties for client_{cid}: {properties}\")\n",
    "\n",
    "\n",
    "    \n",
    "test = pd.concat([test1, test2])\n",
    "test.to_parquet(folder_path + \"test.parquet\")\n",
    "\n",
    "G_test = nx.from_pandas_edgelist(\n",
    "    test, source=dataset1.src_ip_col, target=dataset1.dst_ip_col, create_using=nx.Graph())\n",
    "test_properties = calculate_graph_measures(G_test, \"test\", graphs_properties_path)\n",
    "\n",
    "print(f\"Computed properties for test dataset: {test_properties}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64944281",
   "metadata": {},
   "source": [
    "### Adding Centralities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de87e0cc",
   "metadata": {},
   "source": [
    "Specifying the centralities to add, according to which branch (type) from the four branches of GDLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2105bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_measures_types = [\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_2,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_2,\n",
    "]\n",
    "\n",
    "network_features_types = [\n",
    "    network_features_type_1,\n",
    "    network_features_type_3,\n",
    "    network_features_type_3,\n",
    "    network_features_type_3,\n",
    "    network_features_type_1,\n",
    "    network_features_type_2,\n",
    "    network_features_type_1,\n",
    "    network_features_type_1,\n",
    "    network_features_type_2,\n",
    "]\n",
    "\n",
    "# homogeneous clients, using same centralities \n",
    "\n",
    "# cn_measures_type_0 = [\"betweenness\", \"degree\", \"pagerank\"]\n",
    "# network_features_type_0 = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank']\n",
    "\n",
    "\n",
    "# cn_measures_types = [\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "# ]\n",
    "\n",
    "# network_features_types = [\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a455ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_filenames = [\n",
    "    \"client_0.parquet\",\n",
    "    \"client_1.parquet\",\n",
    "    \"client_2.parquet\",\n",
    "    \"client_3.parquet\",\n",
    "    \"client_4.parquet\",\n",
    "    \"client_5.parquet\",\n",
    "    \"client_6.parquet\",\n",
    "    \"client_7.parquet\",\n",
    "    \"test.parquet\"\n",
    "]\n",
    "clients_paths = [os.path.join(folder_path, name) for name in client_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "679e82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities_columns = []\n",
    "def process_dataset(name, path, dataset, cn_measures, network_features):\n",
    "    print(\"Processing dataset: {}\".format(name))\n",
    "    new_path = os.path.join(folder_path_prep, \"{}.parquet\".format(name))\n",
    "    graph_path = os.path.join(folder_path_prep,\"graphs\",\"graph_{}.gexf\".format(name))\n",
    "    os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(graph_path), exist_ok=True)\n",
    "    \n",
    "    df = pd.read_parquet(path)\n",
    "    # df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # df.dropna(axis=0, how='any', inplace=True)\n",
    "    # df.drop_duplicates(subset=list(set(df.columns) - set([dataset.timestamp_col, dataset.flow_id_col])), keep=\"first\", inplace=True)\n",
    "    \n",
    "    columns = add_centralities(df, new_path, graph_path, dataset, cn_measures, network_features)\n",
    "    centralities_columns.append(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be17222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: client_0\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 4890)\n",
      "==>> features_dicts: ('local_betweenness', 4890)\n",
      "==>> features_dicts: ('degree', 4890)\n",
      "==>> features_dicts: ('local_degree', 4890)\n",
      "==>> features_dicts: ('eigenvector', 4890)\n",
      "==>> features_dicts: ('closeness', 4890)\n",
      "==>> features_dicts: ('pagerank', 4890)\n",
      "==>> features_dicts: ('local_pagerank', 4890)\n",
      "==>> features_dicts: ('k_core', 4890)\n",
      "==>> features_dicts: ('k_truss', 4890)\n",
      "==>> features_dicts: ('Comm', 4890)\n",
      "DataFrame written to temp/preprocessed/client_0.parquet\n",
      "Processing dataset: client_1\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 44)\n",
      "==>> features_dicts: ('local_betweenness', 44)\n",
      "==>> features_dicts: ('pagerank', 44)\n",
      "==>> features_dicts: ('local_pagerank', 44)\n",
      "==>> features_dicts: ('k_core', 44)\n",
      "==>> features_dicts: ('k_truss', 44)\n",
      "==>> features_dicts: ('Comm', 44)\n",
      "DataFrame written to temp/preprocessed/client_1.parquet\n",
      "Processing dataset: client_2\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 46)\n",
      "==>> features_dicts: ('local_betweenness', 46)\n",
      "==>> features_dicts: ('pagerank', 46)\n",
      "==>> features_dicts: ('local_pagerank', 46)\n",
      "==>> features_dicts: ('k_core', 46)\n",
      "==>> features_dicts: ('k_truss', 46)\n",
      "==>> features_dicts: ('Comm', 46)\n",
      "DataFrame written to temp/preprocessed/client_2.parquet\n",
      "Processing dataset: client_3\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 39)\n",
      "==>> features_dicts: ('local_betweenness', 39)\n",
      "==>> features_dicts: ('pagerank', 39)\n",
      "==>> features_dicts: ('local_pagerank', 39)\n",
      "==>> features_dicts: ('k_core', 39)\n",
      "==>> features_dicts: ('k_truss', 39)\n",
      "==>> features_dicts: ('Comm', 39)\n",
      "DataFrame written to temp/preprocessed/client_3.parquet\n",
      "Processing dataset: client_4\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 6531)\n",
      "==>> features_dicts: ('local_betweenness', 6531)\n",
      "==>> features_dicts: ('degree', 6531)\n",
      "==>> features_dicts: ('local_degree', 6531)\n",
      "==>> features_dicts: ('eigenvector', 6531)\n",
      "==>> features_dicts: ('closeness', 6531)\n",
      "==>> features_dicts: ('pagerank', 6531)\n",
      "==>> features_dicts: ('local_pagerank', 6531)\n",
      "==>> features_dicts: ('k_core', 6531)\n",
      "==>> features_dicts: ('k_truss', 6531)\n",
      "==>> features_dicts: ('Comm', 6531)\n",
      "DataFrame written to temp/preprocessed/client_4.parquet\n",
      "Processing dataset: client_5\n",
      "calculated betweenness\n",
      "calculated global_betweenness\n",
      "calculated degree\n",
      "calculated global_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated global_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated mv\n",
      "==>> features_dicts: ('betweenness', 4567)\n",
      "==>> features_dicts: ('global_betweenness', 4567)\n",
      "==>> features_dicts: ('degree', 4567)\n",
      "==>> features_dicts: ('global_degree', 4567)\n",
      "==>> features_dicts: ('eigenvector', 4567)\n",
      "==>> features_dicts: ('closeness', 4567)\n",
      "==>> features_dicts: ('pagerank', 4567)\n",
      "==>> features_dicts: ('global_pagerank', 4567)\n",
      "==>> features_dicts: ('k_core', 4567)\n",
      "==>> features_dicts: ('k_truss', 4567)\n",
      "==>> features_dicts: ('mv', 4567)\n",
      "DataFrame written to temp/preprocessed/client_5.parquet\n",
      "Processing dataset: client_6\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 2703)\n",
      "==>> features_dicts: ('local_betweenness', 2703)\n",
      "==>> features_dicts: ('degree', 2703)\n",
      "==>> features_dicts: ('local_degree', 2703)\n",
      "==>> features_dicts: ('eigenvector', 2703)\n",
      "==>> features_dicts: ('closeness', 2703)\n",
      "==>> features_dicts: ('pagerank', 2703)\n",
      "==>> features_dicts: ('local_pagerank', 2703)\n",
      "==>> features_dicts: ('k_core', 2703)\n",
      "==>> features_dicts: ('k_truss', 2703)\n",
      "==>> features_dicts: ('Comm', 2703)\n",
      "DataFrame written to temp/preprocessed/client_6.parquet\n",
      "Processing dataset: client_7\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 2588)\n",
      "==>> features_dicts: ('local_betweenness', 2588)\n",
      "==>> features_dicts: ('degree', 2588)\n",
      "==>> features_dicts: ('local_degree', 2588)\n",
      "==>> features_dicts: ('eigenvector', 2588)\n",
      "==>> features_dicts: ('closeness', 2588)\n",
      "==>> features_dicts: ('pagerank', 2588)\n",
      "==>> features_dicts: ('local_pagerank', 2588)\n",
      "==>> features_dicts: ('k_core', 2588)\n",
      "==>> features_dicts: ('k_truss', 2588)\n",
      "==>> features_dicts: ('Comm', 2588)\n",
      "DataFrame written to temp/preprocessed/client_7.parquet\n",
      "Processing dataset: test\n",
      "calculated betweenness\n",
      "calculated global_betweenness\n",
      "calculated degree\n",
      "calculated global_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated global_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated mv\n",
      "==>> features_dicts: ('betweenness', 3116)\n",
      "==>> features_dicts: ('global_betweenness', 3116)\n",
      "==>> features_dicts: ('degree', 3116)\n",
      "==>> features_dicts: ('global_degree', 3116)\n",
      "==>> features_dicts: ('eigenvector', 3116)\n",
      "==>> features_dicts: ('closeness', 3116)\n",
      "==>> features_dicts: ('pagerank', 3116)\n",
      "==>> features_dicts: ('global_pagerank', 3116)\n",
      "==>> features_dicts: ('k_core', 3116)\n",
      "==>> features_dicts: ('k_truss', 3116)\n",
      "==>> features_dicts: ('mv', 3116)\n",
      "DataFrame written to temp/preprocessed/test.parquet\n"
     ]
    }
   ],
   "source": [
    "process_dataset(\"client_0\", clients_paths[0], datasets[\"cic_ton_iot_5_percent\"], cn_measures_types[0], network_features_types[0])\n",
    "process_dataset(\"client_1\", clients_paths[1], datasets[\"cic_ton_iot_5_percent\"], cn_measures_types[1], network_features_types[1])\n",
    "process_dataset(\"client_2\", clients_paths[2], datasets[\"cic_ton_iot_5_percent\"], cn_measures_types[2], network_features_types[2])\n",
    "process_dataset(\"client_3\", clients_paths[3], datasets[\"cic_ton_iot_5_percent\"], cn_measures_types[3], network_features_types[3])\n",
    "process_dataset(\"client_4\", clients_paths[4], datasets[\"cic_ton_iot_5_percent\"], cn_measures_types[4], network_features_types[4])\n",
    "process_dataset(\"client_5\", clients_paths[5], datasets[\"cic_ids_2017_5_percent\"], cn_measures_types[5], network_features_types[5])\n",
    "process_dataset(\"client_6\", clients_paths[6], datasets[\"cic_ids_2017_5_percent\"], cn_measures_types[6], network_features_types[6])\n",
    "process_dataset(\"client_7\", clients_paths[7], datasets[\"cic_ids_2017_5_percent\"], cn_measures_types[7], network_features_types[7])\n",
    "process_dataset(\"test\", clients_paths[8], datasets[\"cic_ids_2017_5_percent\"], cn_measures_types[8], network_features_types[8])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a238e436",
   "metadata": {},
   "source": [
    "### Adding PCA columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae855b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Centrality Feature Set Group 1:\n",
      "Centrality Features: {'dst_k_truss', 'src_pagerank', 'dst_Comm', 'dst_local_betweenness', 'src_closeness', 'src_betweenness', 'src_k_truss', 'src_local_betweenness', 'src_k_core', 'dst_local_degree', 'dst_closeness', 'src_eigenvector', 'dst_pagerank', 'dst_k_core', 'src_local_degree', 'dst_local_pagerank', 'src_local_pagerank', 'dst_eigenvector', 'src_degree', 'dst_degree', 'dst_betweenness', 'src_Comm'}\n",
      "Clients: ['temp/preprocessed/client_0.parquet', 'temp/preprocessed/client_4.parquet', 'temp/preprocessed/client_6.parquet', 'temp/preprocessed/client_7.parquet']\n",
      "----------\n",
      "Unique Centrality Feature Set Group 2:\n",
      "Centrality Features: {'dst_k_truss', 'src_pagerank', 'src_Comm', 'dst_k_core', 'dst_Comm', 'dst_local_betweenness', 'src_betweenness', 'src_local_pagerank', 'src_k_truss', 'src_local_betweenness', 'dst_local_pagerank', 'dst_betweenness', 'src_k_core', 'dst_pagerank'}\n",
      "Clients: ['temp/preprocessed/client_1.parquet', 'temp/preprocessed/client_2.parquet', 'temp/preprocessed/client_3.parquet']\n",
      "----------\n",
      "Unique Centrality Feature Set Group 3:\n",
      "Centrality Features: {'dst_k_truss', 'dst_mv', 'src_pagerank', 'dst_global_pagerank', 'src_mv', 'src_global_betweenness', 'src_closeness', 'src_betweenness', 'src_k_truss', 'src_global_pagerank', 'dst_global_betweenness', 'src_k_core', 'src_eigenvector', 'dst_closeness', 'dst_pagerank', 'src_global_degree', 'dst_k_core', 'dst_global_degree', 'dst_eigenvector', 'src_degree', 'dst_degree', 'dst_betweenness'}\n",
      "Clients: ['temp/preprocessed/client_5.parquet', 'temp/preprocessed/test.parquet']\n",
      "----------\n",
      "temp/preprocessed/client_0.parquet\n",
      "temp/preprocessed/client_4.parquet\n",
      "temp/preprocessed/client_6.parquet\n",
      "temp/preprocessed/client_7.parquet\n",
      "temp/preprocessed/client_1.parquet\n",
      "temp/preprocessed/client_2.parquet\n",
      "temp/preprocessed/client_3.parquet\n",
      "temp/preprocessed/client_5.parquet\n",
      "temp/preprocessed/test.parquet\n",
      "Processed federated PCA for client temp/preprocessed/client_0.parquet, saved to datasets/gdlc\\client_0.parquet\n",
      "Client temp/preprocessed/client_0.parquet Local PCA RMSE: 1.3093073414159688\n",
      "Client temp/preprocessed/client_0.parquet Federated PCA RMSE: 1.3093073414159684\n",
      "Processed federated PCA for client temp/preprocessed/client_4.parquet, saved to datasets/gdlc\\client_4.parquet\n",
      "Client temp/preprocessed/client_4.parquet Local PCA RMSE: 1.309307341415903\n",
      "Client temp/preprocessed/client_4.parquet Federated PCA RMSE: 1.309307341415907\n",
      "Processed federated PCA for client temp/preprocessed/client_6.parquet, saved to datasets/gdlc\\client_6.parquet\n",
      "Client temp/preprocessed/client_6.parquet Local PCA RMSE: 1.3093073414159546\n",
      "Client temp/preprocessed/client_6.parquet Federated PCA RMSE: 1.3093073414159553\n",
      "Processed federated PCA for client temp/preprocessed/client_7.parquet, saved to datasets/gdlc\\client_7.parquet\n",
      "Client temp/preprocessed/client_7.parquet Local PCA RMSE: 1.3093073414159153\n",
      "Client temp/preprocessed/client_7.parquet Federated PCA RMSE: 1.3093073414159162\n",
      "Processed federated PCA for client temp/preprocessed/client_1.parquet, saved to datasets/gdlc\\client_1.parquet\n",
      "Client temp/preprocessed/client_1.parquet Local PCA RMSE: 1.3093073414159926\n",
      "Client temp/preprocessed/client_1.parquet Federated PCA RMSE: 1.3093073414159921\n",
      "Processed federated PCA for client temp/preprocessed/client_2.parquet, saved to datasets/gdlc\\client_2.parquet\n",
      "Client temp/preprocessed/client_2.parquet Local PCA RMSE: 1.309307341415796\n",
      "Client temp/preprocessed/client_2.parquet Federated PCA RMSE: 1.3093073414157963\n",
      "Processed federated PCA for client temp/preprocessed/client_3.parquet, saved to datasets/gdlc\\client_3.parquet\n",
      "Client temp/preprocessed/client_3.parquet Local PCA RMSE: 1.30930734141604\n",
      "Client temp/preprocessed/client_3.parquet Federated PCA RMSE: 1.30930734141604\n",
      "Processed federated PCA for client temp/preprocessed/client_5.parquet, saved to datasets/gdlc\\client_5.parquet\n",
      "Client temp/preprocessed/client_5.parquet Local PCA RMSE: 1.3093073414159795\n",
      "Client temp/preprocessed/client_5.parquet Federated PCA RMSE: 1.3093073414159793\n",
      "Processed federated PCA for client temp/preprocessed/test.parquet, saved to datasets/gdlc\\test.parquet\n",
      "Client temp/preprocessed/test.parquet Local PCA RMSE: 1.309307341415959\n",
      "Client temp/preprocessed/test.parquet Federated PCA RMSE: 1.309307341415959\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "all_centrality_measures = set([\n",
    "    'src_degree', 'dst_degree', 'dst_betweenness', 'pagerank', 'dst_local_pagerank',\n",
    "    'src_k_core', 'dst_mv', 'global_betweenness', 'src_local_pagerank', 'dst_local_degree',\n",
    "    'dst_global_pagerank', 'betweenness', 'local_pagerank', 'eigenvector', 'src_global_degree',\n",
    "    'src_global_betweenness', 'src_closeness', 'global_pagerank', 'local_betweenness',\n",
    "    'dst_eigenvector', 'dst_global_betweenness', 'src_global_pagerank', 'k_truss', 'global_degree',\n",
    "    'src_local_degree', 'degree', 'dst_closeness', 'dst_k_truss', 'dst_global_degree', 'dst_pagerank',\n",
    "    'local_degree', 'src_pagerank', 'src_Comm', 'dst_local_betweenness', 'src_k_truss', 'dst_k_core',\n",
    "    'closeness', 'src_mv', 'Comm', 'src_eigenvector', 'dst_Comm', 'src_betweenness', 'k_core',\n",
    "    'src_local_betweenness', 'mv'\n",
    "])\n",
    "\n",
    "clients_paths = [\n",
    "    folder_path_prep + \"client_0.parquet\",\n",
    "    folder_path_prep + \"client_1.parquet\",\n",
    "    folder_path_prep + \"client_2.parquet\",\n",
    "    folder_path_prep + \"client_3.parquet\",\n",
    "    folder_path_prep + \"client_4.parquet\",\n",
    "    folder_path_prep + \"client_5.parquet\",\n",
    "    folder_path_prep + \"client_6.parquet\",\n",
    "    folder_path_prep + \"client_7.parquet\",\n",
    "    folder_path_prep + \"test.parquet\",\n",
    "]\n",
    "\n",
    "client_features = {}\n",
    "\n",
    "for client_path in clients_paths:\n",
    "    df = pd.read_parquet(client_path)\n",
    "    features = set(df.columns)\n",
    "    centrality_features = features.intersection(all_centrality_measures)\n",
    "    client_features[client_path] = centrality_features\n",
    "\n",
    "feature_groups = defaultdict(list)\n",
    "for client_path, features in client_features.items():\n",
    "    feature_groups[frozenset(features)].append(client_path)\n",
    "\n",
    "for i, (unique_feature_set, clients) in enumerate(feature_groups.items(), 1):\n",
    "    print(f\"Unique Centrality Feature Set Group {i}:\")\n",
    "    print(f\"Centrality Features: {set(unique_feature_set)}\")\n",
    "    print(f\"Clients: {clients}\")\n",
    "    print(\"----------\")\n",
    "\n",
    "pca_results, pca_columns = process_clients_with_grouped_pca_rmse(\n",
    "    feature_groups,\n",
    "    output_folder,\n",
    "    n_components=7\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5fc1bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(os.path.join(folder_path, 'labels_names.pkl'), os.path.join(output_folder, '/labels_names.pkl'))\n",
    "with open(output_folder + '/added_columns.pkl', 'wb') as f:\n",
    "    pickle.dump([centralities_columns, pca_columns], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f3df1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clients_paths = [\n",
    "#     output_folder + \"/client_0.parquet\",\n",
    "#     output_folder + \"/client_1.parquet\",\n",
    "#     output_folder + \"/client_2.parquet\",\n",
    "#     output_folder + \"/client_3.parquet\",\n",
    "#     output_folder + \"/client_4.parquet\",\n",
    "#     output_folder + \"/client_5.parquet\",\n",
    "#     output_folder + \"/client_6.parquet\",\n",
    "#     output_folder + \"/client_7.parquet\",\n",
    "#     output_folder + \"/test.parquet\"\n",
    "\n",
    "# ]\n",
    "\n",
    "# evaluate_pca_results(clients_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
